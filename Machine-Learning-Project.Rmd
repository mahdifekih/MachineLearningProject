---
title: "Cluster Analysis of Gambler's Behaviour."
author: "Mahdi Ben Fekih"
output: 
  html_document:
  toc: true
  toc_float: 
    collapsed: true
    smooth_scroll: true 
  toc_depth: 3
date: " `r format(Sys.time(), '%d %B %Y')` "
   
---

```{r setup, include=F, echo=T}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	out.width = "90%"
)
```

# A closer look to the dataset, context, features and description.

## Introduction

The data was collected in the context of an online competition hosted on [Kaggle](https://www.Kaggle.com/), where data on thousands of Bustabit gambling sessions, player's usernames, the amount bet, the amount won, and various properties of the particular game itself were gathered.

## Problem

Understanding human behavior in particular contexts has always been one of the huge challenges in social science fields as well as for businesses that tend to understand their customers in order to grant them a customized experience.

In this particular project, we will draw a special focus on the behavior of online gamblers on a platform called [Bustabit](https://www.bustabit.com/). I will use cluster analysis and decision tree for regression to glean insights into cryptocurrency gambling conduct. The basic rules of this game go as follows:

1. You bet a certain amount of money in a currency entitled **Bits** (1 / 1,000,000th of a Bitcoin) and you win if you cash out before the game crushes or as pronounced in the game **Busts**.

2. Your win is calculated by the multiplier value at the moment you cashed out. For example, if you bet 100 and the value was 2.50x at the time you cashed out, you win 250. In addition, a percentage Bonus per game is multiplied with your bet and summed to give your final Profit in a winning game. Assuming a Bonus of 1%, your Profit for this round would be (100 x 2.5) + (100 x .01) - 100 = 151.

3. The multiplier increases as time goes on, but if you wait too long to cash out, you may bust and lose your money.

To make the concept of the game more understandable, below we see an example of a winning game:

 ![](https://assets.datacamp.com/production/project_643/img/bustabit_win.gif)
 
And a losing example for comparison:

 ![](https://assets.datacamp.com/production/project_643/img/bustabit_loss.gif)
My goal here, is to first define relevant groups or clusters of Bustabit gamblers to identify what online gambling behavior and patterns linger. For instance is it possible to pinpoint users with a risk-averse attitude? Is it possible to spot a set of gamblers that possess a strategy that seems to be more efficient in the long term? Then on a second part I will try to break down the algorithm of the game through a Tree-Based-Model to see if it is possible to predict the profit based on the most relevant features of the dataset.

The data collected includes over 40000 games of Bustabit by a bit over 4000 different players, the data contains the following features:

1. **Id** - Unique identifier for a particular row (game result for one player)

2. **GameID** - Unique identifier for a particular game

3. **Username** - Unique identifier for a particular player

4. **Bet** - The number of Bits (1 / 1,000,000th of a Bitcoin) bet by the player in this game

5. **CashedOut** - The multiplier at which this particular player cashed out

6. **Bonus** - The bonus award (in percent) awarded to this player for the game

7. **Profit** - The amount this player won in the game, calculated as (Bet CashedOut) + (Bet Bonus) - Bet

8. **BustedAt** - The multiplier value at which this game busted

9. **PlayDate** - The date and time at which this game took place

# Data Exploration
 
**Libraries used are:**

```{r data, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
library(readr) #Import the dataset
library(tidyverse) #Used to explore and tidy the data.
library(tidymodels) #Generate the regression tree
library(rsample) #A package to make splitting the dataset into training and testing set easy.
library(dplyr) #used to enable dataframe manipulation.
library(ggplot2) #Basically for every plot.
library(knitr) #Used to knit the markdown document.
library(GGally) #used to produce a Parallel Coordinate Plot to visualize the clusters.
library(cluster) #Used to implement the silhouette method.
```

## Importing and inspecting the data

```{r Load Data, echo=T, message=F}

# Importing the bustabit gambling data 
path <- file.path("~", "Machine Learning Project" , "bustabit.csv")
gamblers <- read.csv(path)

```

Inspecting the structure of the data set:

```{r Inspect Data, echo=T, message=F}
str(gamblers)
dim(gamblers)
```

Our data set is composed of 50000 observations and 9 features.

```{r Head Data, echo=T, message=F}
#Looking at the first five rows of the data.
head(gamblers, n=5)
```
The first 5 observations show a glimpse of the dataset as well as its features.

```{r, echo=T, message=F}
# The highest multiplier (BustedAt value) achieved in a game
gamblers %>%
  arrange(desc(BustedAt)) %>%
  slice(1)
```
The highest multiplier recorded in this dataset is 251025,1.

## Cleaning and Implementing new clustring-friendly features.

As we dive into the dataset, we notice that we need to derive new features to quantify the players' behavior. Currently we only have **Profit** variable that indicates the amount won, we also need to compute the amount lost when a player gets busted, we also need to find a measure to quantify the number of games we won/lost. Thus we will create and modify the following features:

1. **CashedOut** - If the value for CashedOut is NA, we will set it to be 0.01 greater than the BustedAt value to signify that the user failed to cash out before busting

2. **Profit** - If the value for Profit is NA, we will set it to be zero to indicate no profit for the player in that game

3. **Losses** - If the new value for Profit is zero, we will set this to be the amount the player lost in that game, otherwise we will set it to zero. This value should always be zero or negative

4. **GameWon** - If the user made a profit in this game, the value should be 1, and 0 otherwise

5. **GameLost**  If the user had a loss in this game, the value should be 1, and 0 otherwise

```{r ProfitLoss, echo=T}
# Creating new features for clustering
gamblers_features <- gamblers %>% 
  mutate(CashedOut = ifelse(is.na(CashedOut), BustedAt + .01, CashedOut),
         Profit = ifelse(is.na(Profit), 0, Profit),
         Losses = ifelse(Profit == 0, -1 * Bet, 0),
         GameWon = ifelse(Profit == 0, 0, 1),
         GameLost = ifelse(Profit == 0, 1, 0)) 
# A look at the first five rows of the data post edit
head(gamblers_features, n = 5)
```

## Creating per-player statistics
The main challenge at hand is to cluster Bustabit **players** by their individual gambling conduct, nevertheless we find ourselves dealing with features at the per-game level. For instance a single player might have gambled more than one time and had several different outcomes. Hence we to better quantify player behavior we must **group** the data by each player's username so we can start noticing individuals with similar tendencies, the per-player features will be:

1. **AverageCashedOut** - The average multiplier at which the player cashes out

2. **AverageBet** - The average bet made by the player

3. **TotalProfit** - The total profits over time for the player

4. **TotalLosses** - The total losses over time for the player

5. **GamesWon** - The total number of individual games the player won

6. **GamesLost** - The total number of individual games the player lost

With these variables, we will be able to potentially group similar users based on their typical Bustabit gambling behavior.

```{r BustabitByUser, echo=T}
# Group by players to create per-player summary statistics
gamblers_clus <- gamblers_features %>%
  group_by(Username) %>%
  summarize(AverageCashedOut = mean(CashedOut), 
            AverageBet = mean(Bet),
            TotalProfit = sum(Profit),
            TotalLosses = sum(Losses), 
            GamesWon = sum(GameWon),
            GamesLost = sum(GameLost))
# View the first five rows of the data
head(gamblers_clus, n = 5)
```
Now that we grouped the data by each player, the number of observations shrunk to 4149 and  we can clearly distinguish the statistics per player during all gambling sessions, the amount of money yield and lost, the number of matches that ended up as a win/loss and the average multiplier as AverageCashedOut. But as we dive deeper into the observations, we notice that the variables are on different scales thus normalizing the new features will be our next step.

## Scaling and normalization of the derived features

As we brought it up earlier, the variables are on very different **scales** right now. For example, AverageBet is in bits (1/1000000 of a Bitcoin), AverageCashedOut is a multiplier, and GamesLost and GamesWon are counts. As a result, we would like to **normalize** the variables such that across clustering algorithms, they will have approximately equal weighting.

Note that I could compute the Z-scores by using the base R function scale(), but I am going to write my own function in order to get the practice.

```{r Z_Score, echo=T}
# Create the mean-sd standardization function
mean_sd_standard <- function(x) {
  (x-mean(x))/sd(x)
}

# Apply the function to each numeric variable in the clustering set
gamblers_standardized <- gamblers_clus %>%
  mutate_if(is.numeric, mean_sd_standard)

# Summarize our standardized data
summary(gamblers_standardized)
```
The summary of `gamblers_standardized` clearly indicates that all numerical variables have been normalized and scaled.

## Clustering the player data using K means.

With standardized data of per-player features, we are now ready to use K means clustering in order to cluster the players based on their online gambling behavior. K means is implemented in R in the **kmeans()** function from the stats package. This function requires the centers parameter, which represents the number of clusters to use.

Without prior knowledge it is often very difficult to assess the exact number of clusters, taking into consideration this challenge's contingencies, I will begin with **five** clusters. This choice is rather arbitrary but represents a very convenient measure to begin with. We could later use the elbow method for instance to see the exact number of clusters that fit this problem or the silhouette method to compute the measure of 5 clusters.

### Computing the performance of 5 clusters with the silhouette method.
```{r MeasureKmeans, message=FALSE, warning=FALSE, include=T}
# Choose 215092 as our random seed 
set.seed(215092)
#Silhouette analysis: observing the performance level of k=5
Gamblers_k5 <- pam(gamblers_standardized, k=5)
#Gamblers_k5$silinfo$widths to see how does the algorithm work.
```

```{r SilhouetteKmeans, echo=T, include=T}
sil_plot <- silhouette(Gamblers_k5)
plot(sil_plot)
```
According to the silhouette method, the performance level of 5 clusters is above the average, and hence we will approve 5 as the number of clusters.

### Clustering the players using kmeans with five clusters.

```{r RealKmeans, echo=T, include=T}
cluster_solution <- kmeans(select(gamblers_standardized, -Username), centers = 5)
# Store the cluster assignments back into the clustering data frame object
gamblers_clus$cluster <- factor(cluster_solution$cluster)
# Look at the distribution of cluster assignments
table(gamblers_clus$cluster)
```

The k-means algorithm assigned 34 individual to the first cluster, 83 to the second, 453 to the third, 3576 to the fourth and 3 to the fifth cluster. Since we fixed the seed at the beginning of the code chunk above, we are sure that this distribution will not change.

### Visualizing the clusters after assignment using ggplot2 package.

```{r Viz, echo=T, include=T}
ggplot(gamblers_clus, aes(TotalProfit,GamesWon, color=cluster ))+geom_point()+
  scale_x_log10()
```
The plot above clearly depicts the five clusters each with a particular set of individuals whom are characterized with the similar behavior. The question that comes straight into our minds at this step is how to compute an representative behavior of each cluster? Spoiler alert, through average.

## Compute averages for each cluster

We have a clustering assignment which maps every Bustabit gambler to one of five different groups. To begin to assess the quality and distinctiveness of these groups, we are going to look at **group averages** for each cluster across the original variables in our clustering dataset. 

```{r Cluster Analysis, echo=T, message=F}
# Group by the cluster assignment and calculate averages
gamblers_clus_avg <- gamblers_clus %>%
  group_by(cluster) %>%
  summarize_if(is.numeric, mean, na.rm=T) %>%
  arrange(desc(TotalProfit))
# View the resulting table
gamblers_clus_avg

```
This step has allowed us to see that there is a group that makes very large bets, a group that tends to cash out at very high multiplier values, and a group that has played many games of Bustabit. We will dive further into the clusters' distinction in the next step!

## Visualize the clusters with a Parallel Coordinate Plot

We can visualize these group differences graphically using a Parallel Coordinate Plot or PCP. To do so, we will introduce one more kind of scaling: min-max scaling, which forces each variable to fall between 0 and 1.

```{r RandomName, echo=T, include=T}
# Create the min-max scaling function
min_max_standard <- function(x) {
  (x - min(x)) /  (max(x) - min(x) )
}
# Apply this function to each numeric variable in the bustabit_clus_avg object
gamblers_avg_minmax <- gamblers_clus_avg %>%
  mutate_if(is.numeric, min_max_standard)
# Create a parallel coordinate plot of the values
ggparcoord(gamblers_avg_minmax, columns = 2:7, 
           groupColumn = 1, scale = "globalminmax", order = "skewness") + 
  theme(legend.position="bottom") 

```
As a matter of fact, the parallel coordinate plot hasn't granted a meaningful representation here because we have a lot variables. One way to solve this is to use the Principal Components of a dataset.

## Visualize the clusters with Principal Components

In order to reduce the dimensionality to aid in visualization we use the principal components technique. Essentially, this is a two-stage process:

1. We extract the principal components in order to reduce the dimensionality of the dataset so that we can produce a scatterplot in two dimensions that captures the underlying structure of the higher-dimensional data.

2. We then produce a scatterplot of each observation (in this case, each player) across the two Principal Components and color according to their cluster assignment in order to visualize the separation of the clusters.

#

```{r PCA, echo=T}
# Calculate the principal components of the standardized data
my_pc <- as.data.frame(prcomp(select(gamblers_standardized, -Username))$x)
# Store the cluster assignments in the new data frame
my_pc$cluster <- gamblers_clus$cluster
# Use ggplot() to plot PC2 vs PC1, and color by the cluster assignment
p1 <- ggplot(my_pc, aes(x=PC1, y=PC2, color = cluster)) +
  geom_point() +  
  theme(legend.position="bottom") 
# View the resulting plot
p1
```
This plot provides interesting information in terms of the similarity of any two players. In fact, you will see that players who fall close to the boundaries of clusters might be the ones that exhibit the gambling behavior of a couple of different clusters. After you produce your plot, next we will try to determine which clusters seem to be the most "different." Let's try no to figure out each and every cluster.

## Analyzing the groups of gamblers 

Taking in to consideration most of the statistical and programmatical work that as been completed, it is possible now to interpret the Bustabit gambling user groups, so for me I chose the following entitlement for each group:

## Analyzing the groups of gamblers our solution uncovered

Though most of the statistical and programmatical work has been completed, possibly the most important part of a cluster analysis is to interpret the resulting clusters. This often is the most desired aspect of the analysis by clients, who are hoping to use the results of your analysis to inform better business decision making and actionable items. As a final step, we'll use the parallel coordinate plot and cluster means table to interpret the Bustabit gambling user groups! Roughly speaking, we can breakdown the groups as follows:


**Vigilant Players:**

This is the largest of the five clusters, and might be described as the more casual Bustabit players. They've played the fewest number of games overall, and tend to make more conservative bets in general.

**Under Control Addicts:**

These users play a lot of games on Bustabit, but tend to keep their bets under control. As a result, they've made on average a net positive earnings from the site, in spite of having the most games played. They seem to maintain a strategy (or an automated script/bot) that works to earn them money.

**Risk-Loser Gamblers:**

These users seem to be a step above the Cautious Commoners in their Bustabit gambling habits, making larger average bets, and playing a larger number of games on the site. As a result, though they have about the same number of average games won as the Risk Takers, they have a significantly higher number of games lost.

**Risk Takers:**

These users have played only a couple games on average, but their average cashed out value is significantly higher than the other clusters, indicating that they tend to wait for the multiplier to increase to large values before cashing out.

**High Amount Gamblers:**

High bets are the name of the game for this group. They bet large sums of money in each game, although they tend to cash out at lower multipliers and thus play the game more conservatively, particularly compared to the Risk Takers. Interestingly, these users have also on average earned a net positive earnings from their games played.

```{r, echo=T}
cluster_names <- c(
  "Vigilant Players",
  "Risk Takers",
  "Under Control Addicts",
  "High Amount Gamblers",
  "Risky&Loser Gamblers"
)
# Append the cluster names to the cluster means table
gamblers_clus_avg_named <- gamblers_clus_avg %>%
  cbind(Name = cluster_names)
# View the cluster means table with your appended cluster names
gamblers_clus_avg_named
```
As shown in the table above, we finally shed the light on the behaviour of this online game players, and thus we could create similar clusters that encompasses individuals that tend to have the same conduct while gambling online.

# Regression tree

## Purpose of this part

This extra chunk of code is actually dedicated to predicting the TotalProfit based on the other features, more clearly what I will try to do here, is work on a regression tree that aims to spot relations between the outcome which is the TotalProfit and all the the other variables, let's see if it's possible to break down the algorithm of this game!

## Constructing the regression Tree

```{r Constructing Tree, echo=T, message=F}
# Create the specification
tree_spec <- decision_tree(tree_depth=30) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
tree_spec
```
As indicated above,the  Decision Tree Model Specification is set to regression, the standard tree depth is set to 30.

## Train/test split
```{r Split, echo=T, message=F}
# Create the split (rsample package)
gamblers_split <- initial_split(gamblers_clus, prop = 0.8, strata=TotalProfit)
# Print the data split
gamblers_split
```
Luckily we have the rsample package to generate the training set that has 3317 observations, and the testing set that has 832 observations. The strate argument was set to TotalProfit to make both datasets' outcomes balanced. 
```{r Train Test, message=FALSE, warning=FALSE, include=T}
# Creating both training and testing set
gamblers_train <- training(gamblers_split)
gamblers_test <- testing(gamblers_split)
```
## Training Step
```{r training, include=T, message=F}
#Training step
regression_model <- tree_spec %>%
  fit(formula = TotalProfit~AverageBet+
        TotalLosses+GamesWon+GamesLost+
        AverageCashedOut, data=gamblers_train)
```
The regression model is set to be trained to predict the TotalProfit based on the value of AverageBet, TotalLosses, GamesWon, GamesLost, AverageCashedOut.
## Testing Step 
```{r testing, include=T, message=F}
#Testing step
predictions <- predict(regression_model,gamblers_test,) %>%
  bind_cols(gamblers_test)
```
Now that we tested our model, we need to verify the validity of its results. As we know with decision tree for regression we could use RMSE (Residual Mean Square Error RMSE) to compute the reliability of our model.

## The Validity of the model
```{r predicting, include=T, message=F}
#Validity of the model
rmse(predictions, estimate= .pred, truth = TotalProfit)
```
The estimate is highly tremendous, the model is not reliable. Would it be possible this happened because the tree is not tuned? Let's try to tune the regression tree.

## Tuning
```{r Tuninglast, include=T, message=F}
#Creating 10 folds of the training set
gamblers_folds <- vfold_cv(gamblers_train, v = 10)
#creating the tree to be tuned
tree_specTuned <- decision_tree(min_n= tune(),
                                tree_depth=tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
#Implementing the Tuning
tree_grid <- grid_regular(
             parameters(tree_specTuned),
             levels=3 )  
Tune_results <- tune_grid(
  tree_specTuned,
  TotalProfit~AverageBet+
    TotalLosses+GamesWon+GamesLost+
    AverageCashedOut,
  resamples = gamblers_folds,
  grid= tree_grid,
  metrics=metric_set(rmse))
#Use The Best parameters
final_params <- select_best(Tune_results)
best_spec <- finalize_model(tree_specTuned,final_params)
best_spec
```
The tuning indicates that the best parameters are 8 for tree depth, and 2 for min_n.

## Using the tuning parameters 
```{r TuningOr, include=T, message=F}
#Trying with the Tuninig parameters

tree_depth8min2 <- decision_tree(tree_depth=8,min_n=2) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

regression_modelTuned <- tree_depth8min2 %>%
  fit(formula = TotalProfit~AverageBet+
        TotalLosses+GamesWon+GamesLost+
        AverageCashedOut, data=gamblers_train)


predictions2 <- predict(regression_modelTuned,gamblers_test,) %>%
  bind_cols(gamblers_test)
```
## Evaluating the tuning results:
```{r EvalTune, include=T, message=F}

#Evaluate Using Mean Square Error MAE
rmse(predictions2, estimate= .pred, truth = TotalProfit)
```
Again, even with the tuned parameters, we were unable to detect a relation between the profit and the other variables, and thus predict the profit, which indicates that an AI based algorithm is very hard to be broken down with simple machine learning techniques.

# Conclusion

From a business perspective, the clustering has helped us detect key strategies implemented by different individuals on this platform. Bustabit company can leverage those key insights to make more profit. For instance for those following a risk averse strategy and making huge profits out of it, Bustabit ought to make modifications on its algorithm to make it predict this playing technique and inhibit the player from making gains out of this method. Concerning the regression tree part, it is proved that the artificial intelligence has succeeded to overcome the machine learning algorithm that could predict illegitimate ways of overcoming the game rules and make a profit that is not deserved.

